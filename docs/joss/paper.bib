@misc{pyspbla,
    author = {Orachev, Egor and Karpenko, Maria and Alimov, Pavel and Grigorev, Semyon},
    title = {SPbLA: sparse Boolean linear algebra for CPU, Cuda and OpenCL computations},
    year = 2021,
    url = {https://pypi.org/project/pyspbla/},
    note = {Version 1.0.0}
}

@article{article:cfpq_and_rdf_analysis,
    author    = {Xiaowang Zhang and
               Zhiyong Feng and
               Xin Wang and
               Guozheng Rao and
               Wenrui Wu},
    title     = {Context-Free Path Queries on {RDF} Graphs},
    journal   = {CoRR},
    volume    = {abs/1506.00743},
    year      = {2015},
    url       = {http://arxiv.org/abs/1506.00743},
    archivePrefix = {arXiv},
    eprint    = {1506.00743},
    timestamp = {Fri, 20 Mar 2020 11:46:30 +0100},
    biburl    = {https://dblp.org/rec/journals/corr/ZhangFWR15.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    doi       = {10.1007/978-3-319-46523-4_38}
}

@article{article:rna_prediction,
    author = {Anderson, James and Novák, Adám and Sükösd, Zsuzsanna and Golden, Michael and Arunapuram, Preeti and Edvardsson, Ingolfur and Hein, Jotun},
    year = {2013},
    month = {05},
    pages = {149},
    title = {Quantifying variances in comparative RNA secondary structure prediction},
    volume = {14},
    journal = {BMC bioinformatics},
    doi = {10.1186/1471-2105-14-149}
}

@article{article:dyck_cfl_code_analysis,
    author = {Zhang, Qirun and Lyu, Michael R. and Yuan, Hao and Su, Zhendong},
    title = {Fast Algorithms for Dyck-CFL-Reachability with Applications to Alias Analysis},
    year = {2013},
    issue_date = {June 2013},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {48},
    number = {6},
    issn = {0362-1340},
    url = {https://doi.org/10.1145/2499370.2462159},
    doi = {10.1145/2499370.2462159},
    abstract = {The context-free language (CFL) reachability problem is a well-known fundamental formulation in program analysis. In practice, many program analyses, especially pointer analyses, adopt a restricted version of CFL-reachability, Dyck-CFL-reachability, and compute on edge-labeled bidirected graphs. Solving the all-pairs Dyck-CFL-reachability on such bidirected graphs is expensive. For a bidirected graph with n nodes and m edges, the traditional dynamic programming style algorithm exhibits a subcubic time complexity for the Dyck language with k kinds of parentheses. When the underlying graphs are restricted to bidirected trees, an algorithm with O(n log n log k) time complexity was proposed recently. This paper studies the Dyck-CFL-reachability problems on bidirected trees and graphs. In particular, it presents two fast algorithms with O(n) and O(n + m log m) time complexities on trees and graphs respectively. We have implemented and evaluated our algorithms on a state-of-the-art alias analysis for Java. Results on standard benchmarks show that our algorithms achieve orders of magnitude speedup and consume less memory.},
    journal = {SIGPLAN Not.},
    month = jun,
    pages = {435–446},
    numpages = {12},
    keywords = {alias analysis, dyck-cfl-reachability}
}

@inproceedings{inproceedings:matrix_cfpq,
    author = {Azimov, Rustam and Grigorev, Semyon},
    year = {2018},
    month = {06},
    pages = {1-10},
    title = {Context-free path querying by matrix multiplication},
    doi = {10.1145/3210259.3210264}
}

@inbook{inbook:kronecker_cfpq_adbis,
    author = {Orachev, Egor and Epelbaum, Ilya and Azimov, Rustam and Grigorev, Semyon},
    year = {2020},
    month = {08},
    pages = {49-59},
    title = {Context-Free Path Querying by Kronecker Product},
    isbn = {978-3-030-54831-5},
    doi = {10.1007/978-3-030-54832-2_6}
}

% SuiteSparse for solving graph problems
@article{article:suite_sparse_for_graph_problems,
    author = {Davis, Timothy A.},
    title = {Algorithm 1000: SuiteSparse:GraphBLAS: Graph Algorithms in the Language of Sparse Linear Algebra},
    year = {2019},
    issue_date = {December 2019},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {45},
    number = {4},
    issn = {0098-3500},
    url = {https://doi.org/10.1145/3322125},
    doi = {10.1145/3322125},
    abstract = {SuiteSparse:GraphBLAS is a full implementation of the GraphBLAS standard, which defines a set of sparse matrix operations on an extended algebra of semirings using an almost unlimited variety of operators and types. When applied to sparse adjacency matrices, these algebraic operations are equivalent to computations on graphs. GraphBLAS provides a powerful and expressive framework for creating graph algorithms based on the elegant mathematics of sparse matrix operations on a semiring. An overview of the GraphBLAS specification is given, followed by a description of the key features and performance of its implementation in the SuiteSparse:GraphBLAS package.},
    journal = {ACM Trans. Math. Softw.},
    month = dec,
    articleno = {44},
    numpages = {25},
    keywords = {Graph algorithms, GraphBLAS, sparse matrices}
}

@misc{pygraphblas,
    title      = "pygraphblas: a Python wrapper around the GraphBLAS API",
    howpublished = "Github",
    year       = 2021,
    url        = {https://github.com/Graphegon/pygraphblas},
    urldate    = "29.06.2021",
    language   = "english"
}

% cuSPARSE reference
@MISC{net:cusparse_docs,
    title       = {Sparse matrix library in Cuda},
    url         = {https://docs.nvidia.com/cuda/cusparse/},
    urldate     = "16.04.2021",
    language    = "english"
}

% bhSPARSE
@article{10.1016/j.jpdc.2015.06.010,
    author = {Liu, Weifeng and Vinter, Brian},
    title = {A Framework for General Sparse Matrix-Matrix Multiplication on GPUs and Heterogeneous Processors},
    year = {2015},
    issue_date = {November 2015},
    publisher = {Academic Press, Inc.},
    address = {USA},
    volume = {85},
    number = {C},
    issn = {0743-7315},
    url = {https://doi.org/10.1016/j.jpdc.2015.06.010},
    doi = {10.1016/j.jpdc.2015.06.010},
    abstract = {General sparse matrix-matrix multiplication (SpGEMM) is a fundamental building block for numerous applications such as algebraic multigrid method (AMG), breadth first search and shortest path problem. Compared to other sparse BLAS routines, an efficient parallel SpGEMM implementation has to handle extra irregularity from three aspects: (1) the number of nonzero entries in the resulting sparse matrix is unknown in advance, (2) very expensive parallel insert operations at random positions in the resulting sparse matrix dominate the execution time, and (3) load balancing must account for sparse data in both input matrices.In this work we propose a framework for SpGEMM on GPUs and emerging CPU-GPU heterogeneous processors. This framework particularly focuses on the above three problems. Memory pre-allocation for the resulting matrix is organized by a hybrid method that saves a large amount of global memory space and efficiently utilizes the very limited on-chip scratchpad memory. Parallel insert operations of the nonzero entries are implemented through the GPU merge path algorithm that is experimentally found to be the fastest GPU merge approach. Load balancing builds on the number of necessary arithmetic operations on the nonzero entries and is guaranteed in all stages.Compared with the state-of-the-art CPU and GPU SpGEMM methods, our approach delivers excellent absolute performance and relative speedups on various benchmarks multiplying matrices with diverse sparsity structures. Furthermore, on heterogeneous processors, our SpGEMM approach achieves higher throughput by using re-allocatable shared virtual memory. We design a framework for SpGEMM on modern manycore processors using the CSR format.We present a hybrid method for pre-allocating the resulting sparse matrix.We propose an efficient parallel insert method for long rows of the resulting matrix.We develop a heuristic-based load balancing strategy.Our approach significantly outperforms other known CPU and GPU SpGEMM methods.},
    journal = {J. Parallel Distrib. Comput.},
    month = nov,
    pages = {47--61},
    numpages = {15},
    keywords = {Linear algebra, Sparse matrix, Sparse matrix-matrix multiplication, Heterogeneous processor, GPU, Merging, Parallel algorithm}
}

% Graphblast
@article{yang2019graphblast,
    title = {{GraphBLAST}: A High-Performance Linear Algebra-based Graph Framework on the {GPU}},
    author = {Carl Yang and Ayd{\i}n Bulu{\c{c}} and John D. Owens},
    year = {2019},
    journal = {arXiv preprint},
    arxiv = {https://arxiv.org/abs/1908.01407}
}

% Cusplibrary project
@MISC{net:cusplibrary,
    author = "Steven Dalton and Nathan Bell and Luke Olson and Michael Garland",
    title = "Cusp: Generic Parallel Algorithms for Sparse Matrix and Graph Computations",
    year = "2014",
    url = "http://cusplibrary.github.io/",
    note = "Version 0.5.0"
}

% clSPARSE project
@inproceedings{10.1145/2909437.2909442,
    author = {Greathouse, Joseph L. and Knox, Kent and Po\l{}a, Jakub and Varaganti, Kiran and Daga, Mayank},
    title = {ClSPARSE: A Vendor-Optimized Open-Source Sparse BLAS Library},
    year = {2016},
    isbn = {9781450343381},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2909437.2909442},
    doi = {10.1145/2909437.2909442},
    abstract = {Sparse linear algebra is a cornerstone of modern computational science. These algorithms ignore the zero-valued entries found in many domains in order to work on much larger problems at much faster rates than dense algorithms. Nonetheless, optimizing these algorithms is not straightforward. Highly optimized algorithms for multiplying a sparse matrix by a dense vector, for instance, are the subject of a vast corpus of research and can be hundreds of times longer than na\"{\i}ve implementations. Optimized sparse linear algebra libraries are thus needed so that users can build applications without enormous effort.Hardware vendors release proprietary libraries that are highly optimized for their devices, but they limit interoperability and promote vendor lock-in. Open libraries often work across multiple devices and can quickly take advantage of new innovations, but they may not reach peak performance. The goal of this work is to provide a sparse linear algebra library that offers both of these advantages.We thus describe clSPARSE, a permissively licensed open-source sparse linear algebra library that offers state-of-the-art optimized algorithms implemented in OpenCL™. We test clSPARSE on GPUs from AMD and Nvidia and show performance benefits over both the proprietary cuSPARSE library and the open-source ViennaCL library.},
    booktitle = {Proceedings of the 4th International Workshop on OpenCL},
    articleno = {7},
    numpages = {4},
    keywords = {GPGPU, OpenCL, Sparse Linear Algebra, clSPARSE},
    location = {Vienna, Austria},
    series = {IWOCL '16}
}

@INPROCEEDINGS{paper:graphblas_foundations,
    author={J. {Kepner} and P. {Aaltonen} and D. {Bader} and A. {Buluc} and F. {Franchetti} and J. {Gilbert} and D. {Hutchison} and M. {Kumar} and A. {Lumsdaine} and H. {Meyerhenke} and S. {McMillan} and C. {Yang} and J. D. {Owens} and M. {Zalewski} and T. {Mattson} and J. {Moreira}},
    booktitle={2016 IEEE High Performance Extreme Computing Conference (HPEC)},
    title={Mathematical foundations of the GraphBLAS},
    year={2016},
    volume={},
    number={},
    pages={1--9},
    keywords={graph theory;mathematics computing;matrix algebra;programming environments;mathematical foundations;GraphBLAS standard;GraphBlas.org;matrix-based graph algorithms;matrix-based graph operations;programming environments;adjacency matrices;incidence matrices;matrix multiplication;matrix mathematics;Matrices;Sparse matrices;Finite element analysis;Standards;Additives},
    doi={10.1109/HPEC.2016.7761646},
    ISSN={},
    month={Sep.}
}

% SuiteSparse Sparse Matrix Collection
@MISC{data:suitesparse_matrix_collection,
  author = {Davis, T.},
  title = {SuiteSparse Matrix Collection (the University of Florida Sparse Matrix Collection)},
  url = {https://sparse.tamu.edu/}
}